{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path_to_tags = 'D:/Nika/3/lingstylo/stanza_parsed_texts/corpus'\n",
    "\n",
    "path_to_allposgram = 'D:/Nika/3/lingstylo/stanza_allposgram/corpus'\n",
    "path_to_pos = 'D:/Nika/3/lingstylo/stanza_new_pos/corpus'\n",
    "path_to_ud = 'D:/Nika/3/lingstylo/stanza_new_ud/corpus'\n",
    "path_to_udgram = 'D:/Nika/3/lingstylo/stanza_new_udgram/corpus'\n",
    "path_to_udgrambiord = 'D:/Nika/3/lingstylo/stanza_new_udgrambiord/corpus'\n",
    "path_to_udgrambitree = 'D:/Nika/3/lingstylo/stanza_new_udgrambitree/corpus'\n",
    "\n",
    "'''all_paths = [path_to_allposgram, path_to_pos, path_to_ud, \n",
    "             path_to_udgram, path_to_udgrambiord, path_to_udgrambitree]'''\n",
    "all_paths = [path_to_udgrambiord, path_to_udgrambitree]\n",
    "\n",
    "for one_path in all_paths:\n",
    "    if os.path.exists(one_path) is False:\n",
    "        os.makedirs(one_path)\n",
    "\n",
    "\n",
    "#os.makedirs(path_to_result)\n",
    "#os.makedirs(path_to_tags)\n",
    "#a = set()\n",
    "for path, dirs, files in os.walk('D:/Nika/3/lingstylo/stanza_parsed_texts/corpus'):\n",
    "    for file in files:\n",
    "        test(path, file, path_to_tags, all_paths)\n",
    "        #a = a.union(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upos = []\n",
    "deprel = []\n",
    "\n",
    "for i in list(a):\n",
    "    upos.append(i.upos)\n",
    "    deprel.append(i.deprel)\n",
    "set(deprel)\n",
    "set(upos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "from stanza.models.common.doc import Document\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(path, file, path_to_tags, all_paths):\n",
    "    \n",
    "    #path_to_allposgram, path_to_pos, path_to_ud, path_to_udgram, \n",
    "    path_to_udgrambiord, path_to_udgrambitree = all_paths\n",
    "    \n",
    "    with open(path_to_tags + '/' + file, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    all_tags_text = Document(json.loads(data))\n",
    "    \n",
    "    '''pos_list = []\n",
    "    posgram_list = []\n",
    "    ud_list = []\n",
    "    udgram_list = []'''\n",
    "    udgrambitree_list = []\n",
    "    \n",
    "    #strange_list = []\n",
    "\n",
    "    for sent in all_tags_text.sentences:\n",
    "        for word in sent.words:\n",
    "            #postags = ''\n",
    "            #if word.feats:\n",
    "                #postags = ''.join((i.split('=')[1] for i in word.feats.split('|')))\n",
    "                #postags = re.sub(r'3', 'third', postags)\n",
    "                #postags = re.sub(r'2', 'second', postags)\n",
    "                #postags = re.sub(r'1', 'first', postags)\n",
    "                \n",
    "            #posgram_list.append(word.upos + postags)\n",
    "            #pos_list.append(word.upos)\n",
    "            #ud_list.append(word.deprel)\n",
    "            \n",
    "            if word.head == 0:\n",
    "                headtag = 'root'\n",
    "            else:\n",
    "                for w in sent.words:\n",
    "                    if w.id == word.head:\n",
    "                        headtag = w.deprel\n",
    "            \n",
    "            udgrambitree_list.append(word.deprel + headtag)\n",
    "\n",
    "    \n",
    "\n",
    "    '''with open(path_to_allposgram + '/' + file, 'w', encoding='utf-8') as f:\n",
    "        f.write(' '.join(posgram_list))\n",
    "\n",
    "    with open(path_to_pos + '/' + file, 'w', encoding='utf-8') as f:\n",
    "        f.write(' '.join(pos_list))\n",
    "        \n",
    "    with open(path_to_ud + '/' + file, 'w', encoding='utf-8') as f:\n",
    "        f.write(' '.join(ud_list)) '''     \n",
    "        \n",
    "    with open(path_to_udgrambitree + '/' + file, 'w', encoding='utf-8') as f:\n",
    "        f.write(' '.join(udgrambitree_list))\n",
    "        \n",
    "        \n",
    "    #return set(strange_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(path, file, path_to_tags, all_paths):\n",
    "    \n",
    "    path_to_allposgram, path_to_pos, path_to_ud, path_to_udgram, path_to_udgrambiord, path_to_udgrambitree = all_paths\n",
    "    \n",
    "    with open(path_to_tags + '/' + file, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    all_tags_text = Document(json.loads(data))\n",
    "    \n",
    "    pos_list = []\n",
    "    posgram_list = []\n",
    "    ud_list = []\n",
    "    udgram_list = []\n",
    "    \n",
    "    #strange_list = []\n",
    "\n",
    "    for sent in all_tags_text.sentences:\n",
    "        for word in sent.words:\n",
    "            postags = ''\n",
    "            if word.feats:\n",
    "                postags = ''.join((i.split('=')[1] for i in word.feats.split('|')))\n",
    "                postags = re.sub(r'3', 'third', postags)\n",
    "                postags = re.sub(r'2', 'second', postags)\n",
    "                postags = re.sub(r'1', 'first', postags)\n",
    "                \n",
    "            posgram_list.append(word.upos + postags)\n",
    "            pos_list.append(word.upos)\n",
    "            ud_list.append(word.deprel)\n",
    "            udgram_list.append(word.deprel + word.upos + postags)\n",
    "\n",
    "    \n",
    "\n",
    "    with open(path_to_allposgram + '/' + file, 'w', encoding='utf-8') as f:\n",
    "        f.write(' '.join(posgram_list))\n",
    "\n",
    "    with open(path_to_pos + '/' + file, 'w', encoding='utf-8') as f:\n",
    "        f.write(' '.join(pos_list))\n",
    "        \n",
    "    with open(path_to_ud + '/' + file, 'w', encoding='utf-8') as f:\n",
    "        f.write(' '.join(ud_list))        \n",
    "        \n",
    "    with open(path_to_udgram + '/' + file, 'w', encoding='utf-8') as f:\n",
    "        f.write(' '.join(udgram_list))\n",
    "        \n",
    "        \n",
    "    #return set(strange_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-15 18:28:51 INFO: Loading these models for language: ru (Russian):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | syntagrus |\n",
      "| pos       | syntagrus |\n",
      "| lemma     | syntagrus |\n",
      "| depparse  | syntagrus |\n",
      "=========================\n",
      "\n",
      "2021-05-15 18:28:51 INFO: Use device: cpu\n",
      "2021-05-15 18:28:51 INFO: Loading: tokenize\n",
      "2021-05-15 18:28:51 INFO: Loading: pos\n",
      "2021-05-15 18:28:54 INFO: Loading: lemma\n",
      "2021-05-15 18:28:55 INFO: Loading: depparse\n",
      "2021-05-15 18:28:57 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 1\tword: Волга\thead id: 2\thead: впадает\tdeprel: nsubj\n",
      "id: 2\tword: впадает\thead id: 0\thead: root\tdeprel: root\n",
      "id: 3\tword: в\thead id: 5\thead: море\tdeprel: case\n",
      "id: 4\tword: Каспийское\thead id: 5\thead: море\tdeprel: amod\n",
      "id: 5\tword: море\thead id: 2\thead: впадает\tdeprel: obl\n",
      "id: 6\tword: .\thead id: 2\thead: впадает\tdeprel: punct\n",
      "id: 1\tword: Новое\thead id: 2\thead: время\tdeprel: amod\n",
      "id: 2\tword: время\thead id: 0\thead: root\tdeprel: root\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='ru', processors='tokenize,pos,lemma,depparse')\n",
    "doc = nlp('''Волга впадает в Каспийское море.\n",
    "Новое время''')\n",
    "print(*[f'id: {word.id}\\tword: {word.text}\\thead id: {word.head}\\thead: {sent.words[word.head-1].text if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"id\": 1,\n",
       "  \"text\": \"Волга\",\n",
       "  \"lemma\": \"Волга\",\n",
       "  \"upos\": \"NOUN\",\n",
       "  \"feats\": \"Animacy=Inan|Case=Nom|Gender=Fem|Number=Sing\",\n",
       "  \"head\": 2,\n",
       "  \"deprel\": \"nsubj\",\n",
       "  \"misc\": \"start_char=0|end_char=5\"\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.sentences[0].words[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
